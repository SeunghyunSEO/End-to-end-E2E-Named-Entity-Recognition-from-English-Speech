{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import json\n",
    "pos_tag=[]\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating sentence-[prdiction list] ouput for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=os.listdir(\"txt_2/txt\")\n",
    "x_list=[]\n",
    "for file in file_list:\n",
    "    f=open(\"txt_2/txt/\"+file, \"r\")\n",
    "    x_list.append(f.read())\n",
    "    \n",
    "test=pd.DataFrame(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for length if needed\n",
    "test['total_length'] = test[0].apply(len)\n",
    "test.drop(\"total_length\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding output to sentences from LSTM-CRF\n",
    "with open(\"targer-master/out.json\") as json_file:\n",
    "    data=json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_output=pd.DataFrame()\n",
    "formatted_output[\"sentences\"]=test[0]\n",
    "formatted_output[\"tags\"]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'formatted_output.txt', formatted_output.values, fmt='%s', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing from last - making CoNLL-2003 formatted data from simple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_num=[]\n",
    "character=[]\n",
    "for i,row in test.iterrows():\n",
    "    character.append(\"null\")\n",
    "    sentence_num.append(i)\n",
    "\n",
    "    for j in str(row[0]).split():\n",
    "        sentence_num.append(i)\n",
    "        character.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final[\"character\"]=character\n",
    "test_final[\"sentence_num\"]=sentence_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final[\"character\"].replace(\"null\",np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final[\"character\"]=test_final[\"character\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_final.to_csv(\"test.txt\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag=[]\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final.fillna(\"-\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15248"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(test_final[\"character\"].values))\n",
    "n_words = len(words); n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words=test_final['character'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding POS to CoNLL formatted data\n",
    "pos_tag={}\n",
    "for ind in tqdm(range(len(unique_words))):\n",
    "    doc=nlp(unique_words[ind])\n",
    "    for token in doc:\n",
    "        pos_tag[unique_words[ind]]=token.tag_\n",
    "        \n",
    "        \n",
    "train_words=list(pos_tag.keys())\n",
    "train_pos=list(pos_tag.values())\n",
    "\n",
    "pos_df=pd.DataFrame()\n",
    "pos_df['character']=train_words\n",
    "pos_df['pos']=train_pos\n",
    "\n",
    "train_df_pos=test_final.merge(pos_df,on='character',how='left')\n",
    "# Converting text to lower-case\n",
    "train_df_pos[\"character\"]=train_df_pos[\"character\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index, row in train_df_pos.iterrows():\n",
    "#    if row[\"character\"]==\"-\":\n",
    "#        row[\"character\"][index]=np.nan\n",
    "#        row[\"pos\"][index]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_pos[\"character\"].replace(\"-\",np.nan,inplace=True)\n",
    "train_df_pos[\"pos\"].replace(\":\",np.nan,inplace=True)\n",
    "train_df_pos.fillna(\"\",inplace=True)\n",
    "train_df_pos[\"tag1\"]=\"O\"\n",
    "train_df_pos[\"tag2\"]=\"O\"\n",
    "train_df_pos.loc[(train_df_pos['pos'] == \"\"), 'tag1'] = \"\"\n",
    "train_df_pos.loc[(train_df_pos['pos'] == \"\"), 'tag2'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_pos.drop(\"sentence_num\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_final.to_csv(\"test.txt\",index=False)\n",
    "np.savetxt(r'test_big.txt', train_df_pos.values, fmt='%s', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random - Take only important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"targer-master/out.json\") as json_file:\n",
    "    data=json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=os.listdir(\"txt_2/txt\")\n",
    "x_list=[]\n",
    "for file in file_list:\n",
    "    f=open(\"txt_2/txt/\"+file, \"r\")\n",
    "    x_list.append(f.read())\n",
    "    \n",
    "test=pd.DataFrame(x_list)\n",
    "\n",
    "sentence_num_2=[]\n",
    "character_2=[]\n",
    "for i,row in test.iterrows():\n",
    "    #character.append(\"null\")\n",
    "    #sentence_num_2.append(i)\n",
    "\n",
    "    for j in str(row[0]).split():\n",
    "        sentence_num_2.append(i)\n",
    "        character_2.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [y for x in data for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "op=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "op[\"character\"]=character_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "op[\"predicted_label\"]=flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "value=op[op[\"predicted_label\"]==\"B-PER\"][\"character\"].value_counts()\n",
    "value=dict(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_occuring=[]\n",
    "#for item in value:\n",
    "#    if value[item]>50:\n",
    "#        large_occuring.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_occuring=[]\n",
    "for item in value:\n",
    "    all_occuring.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "include=['MARTIN','DICK','ELIJAH''HARRISON','IDEALISM','CAMERON','MANUEL''GREGSON','JONES',\"O'BRIEN\",'PAT','DAYLIGHT','NIMROD','PIERRE','LORD',\"JEANNE'S\",'BILL','DAUGHTRY',\"SAXON'S\",'ELI','PASCAL','STEWART','ERNEST''OOLONG',\n",
    " 'OUTWARDLY',\"DENNIN'S\",'SANDEL','GOD','MACDOUGALL','FRANCOIS','SAXON','FERGUSON','JESSE','ISAAC','STEVE','CURLY','HARRY','GEORGE','ARTHUR','PHILIP','WHITE'\"DOANE'S\",\n",
    " 'JOHNNY','BOB','PHIL','BURKE','JACQUES','SARAH','JERRY','GABRIEL','WHITTEMORE','THORPE','JOE','TOM','CHAUNCEY','CHEROKEE''SHORTY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=list(set(all_occuring).difference(set(include)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=[\"YOU'LL\",\"YOU'RE\",\"YOU'VE\",\"WE'LL\",'SIR',\"SHOULDN'T\",\"SHE'D\",'ORGANIZED','MR','MRS','MOTHER','MATHEMATICAL','MADAM',\"MA'AM\",\n",
    "'LADY','IGNORANTLY',\"I'LL\",\"I'M\",\"I'VE\",'HYPOTHESES',\"HE'S\",\"HE'LL\",'HELLO',\"HADN'T\",'FOURTEEN','ELDORADO','DEJECTED','COOK',\n",
    "\"CHILDREN'S\",\"BOY'S\",'ANYWAYS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in op.iterrows():\n",
    "    if row[\"character\"] in exclude:\n",
    "        op[\"predicted_label\"].iloc[index]=\"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#op[\"predicted_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#op[op[\"predicted_label\"]==\"B-ORG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "op[\"sentence_num\"]=sentence_num_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#op[\"predicted_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#op[op[\"predicted_label\"]==\"B-PER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#op.to_csv(\"Final_Output.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol=False\n",
    "index1=0\n",
    "hemant=[]\n",
    "for index,row in op.iterrows():\n",
    "    index1+=1\n",
    "#for ind in op.index:\n",
    "    if row[\"predicted_label\"]==\"B-PER\":\n",
    "        #line = pd.DataFrame({\"character\": \"|\", \"predicted_label\": \"O\",\"sentence_num\":-999}, index=[index-1])\n",
    "        #op = pd.concat([op.iloc[:index], line, op.iloc[index:]]).reset_index(drop=True)\n",
    "        #i+=2\n",
    "        op = pd.DataFrame(np.insert(op.values, index1-1, values=[\"|\",\"O\",row[\"sentence_num\"]], axis=0),columns = op.columns)\n",
    "        op = pd.DataFrame(np.insert(op.values, index1+1, values=[\">\",\"O\",row[\"sentence_num\"]], axis=0),columns = op.columns)\n",
    "        index1+=2\n",
    "        hemant.append(row[\"sentence_num\"])\n",
    "        \n",
    "        #print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#op[op[\"predicted_label\"]==\"B-PER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#op[op[\"sentence_num\"]==8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#op.iloc[450:470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentences from CoNLL-2003 formatted data\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [w for w in s['character'].values.tolist()]\n",
    "        self.grouped = self.data.groupby('sentence_num').apply(agg_func)\n",
    "        self.sentences = self.grouped\n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            return None\n",
    "getter = SentenceGetter(op)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_final=[]\n",
    "for i in range(len(sentences)):\n",
    "    sentences_final.append(' '.join(word for word in sentences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.DataFrame()\n",
    "final[\"sentences\"]=sentences_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_final_2=list(final[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=os.listdir(\"txt_2/txt/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemant1=list(set(hemant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_hemant=[]\n",
    "for i in range(len(file_list)):\n",
    "    if i in hemant1:\n",
    "        file_name_hemant.append(file_list[i])\n",
    "    f = open(\"Updated_txt_big_95_2/\"+file_list[i],'w')\n",
    "    f.write(sentences_final_2[i])\n",
    "    #np.savetxt(r\"Updated_txt/\"+file_list[i], sentences_final_2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('filenames.txt', 'w') as f:\n",
    "    for item in file_name_hemant:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit CoNLL-2003 dataset using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lol = pd.read_fwf(\"C:/Users/Sreyan/Desktop/corpus_mixed/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "file=open(\"C:/Users/Sreyan/Desktop/NER/targer-master/data/NER/CoNNL_2003_shared_task/train.txt\",\"r\")\n",
    "lines=file.readlines()\n",
    "new_data=pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=new_data[0].str.split(\" \",n=3,expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=columns.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns[0].replace(\"\\n\",\" \",inplace=True)\n",
    "columns[1] = [str(i or ' ') for i in columns[1]]\n",
    "columns[2] = [str(i or ' ') for i in columns[2]]\n",
    "columns[3] = [str(i or ' ') for i in columns[3]]\n",
    "#columns[1] = [str(i or ' ') for i in columns[1]]\n",
    "#columns[1].replace(None,\" \",inplace=True)\n",
    "#columns[2].replace(None,\" \",inplace=True)\n",
    "#columns[3].replace(None,\" \",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns[3] = columns[3].map(lambda x: x.lstrip('').rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns[0]=columns[0].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add and drop columns if required\n",
    "columns.drop([1,2],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra code for removing MISC NER Tags\n",
    "columns[3].replace(\"B-MISC\",\"O\",inplace=True)\n",
    "columns[3].replace(\"I-MISC\",\"O\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns according to requirement\n",
    "columns.rename(columns = {0: \"character\", 1:\"ner\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset after editing\n",
    "np.savetxt(r\"C:/Users/Sreyan/Desktop/conll3.txt\", columns.values, fmt='%s', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Creating WNUT NER Data fit for adding to CoNLL-2003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text file\n",
    "file_wnut=open(\"C:/Users/Sreyan/Desktop/NER/NER-datasets-master/WNUT17/wnut_dev.txt\",\"r\",encoding=\"utf8\")\n",
    "lines_wnut=file_wnut.readlines()\n",
    "new_data_wnut=pd.DataFrame(lines_wnut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_wnut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_wnut=new_data_wnut[0].str.split(\"\\t\",n=2,expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making it fit for editing\n",
    "columns_wnut[0].replace(\"\\n\",\" \",inplace=True)\n",
    "columns_wnut[1] = [str(i or ' ') for i in columns_wnut[1]]\n",
    "#columns_wnut[2] = [str(i or ' ') for i in columns_wnut[2]]\n",
    "#columns_wnut[3] = [str(i or ' ') for i in columns_wnut[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_wnut[1] = columns_wnut[1].map(lambda x: x.lstrip('').rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_wnut[0]=columns_wnut[0].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editing NER Tags to make it similar to CoNLL-2003\n",
    "columns_wnut[1].replace(\"B-person\",\"B-PER\",inplace=True)\n",
    "columns_wnut[1].replace(\"I-person\",\"I-PER\",inplace=True)\n",
    "columns_wnut[1].replace(\"I-location\",\"I-LOC\",inplace=True)\n",
    "columns_wnut[1].replace(\"B-location\",\"B-LOC\",inplace=True)\n",
    "columns_wnut[1].replace(\"B-corporation\",\"B-ORG\",inplace=True)\n",
    "columns_wnut[1].replace(\"I-corporation\",\"I-ORG\",inplace=True)\n",
    "columns_wnut[1].replace(\"I-product\",\"O\",inplace=True)\n",
    "columns_wnut[1].replace(\"B-product\",\"O\",inplace=True)\n",
    "columns_wnut[1].replace(\"I-group\",\"O\",inplace=True)\n",
    "columns_wnut[1].replace(\"B-group\",\"O\",inplace=True)\n",
    "columns_wnut[1].replace(\"B-creative-work\",\"O\",inplace=True)\n",
    "columns_wnut[1].replace(\"I-creative-work\",\"O\",inplace=True)\n",
    "columns_wnut[1].replace(\"\",\" \",inplace=True)\n",
    "columns_wnut[1].replace(\"-\",\"O\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_wnut.replace(\" \",\"-\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_wnut[\"character\"]=columns_wnut[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3539"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(columns_wnut[0].values))\n",
    "n_words = len(words); n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words=columns_wnut[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3539/3539 [00:20<00:00, 174.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating POS for words\n",
    "pos_tag={}\n",
    "for ind in tqdm(range(len(unique_words))):\n",
    "    doc=nlp(unique_words[ind])\n",
    "    for token in doc:\n",
    "        pos_tag[unique_words[ind]]=token.tag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words=list(pos_tag.keys())\n",
    "train_pos=list(pos_tag.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df=pd.DataFrame()\n",
    "pos_df['character']=train_words\n",
    "pos_df['pos']=train_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging with original words\n",
    "wnut=columns_wnut.merge(pos_df,on='character',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut[\"tag\"]=wnut[1]\n",
    "wnut.drop([0,1],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in wnut.iterrows():\n",
    "    if row[\"pos\"]==\":\":\n",
    "        wnut[\"character\"].iloc[index]=\" \"\n",
    "        wnut[\"tag\"].iloc[index]=\" \"\n",
    "        wnut[\"pos\"].iloc[index]=\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: character, dtype: int64)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut[wnut[\"tag\"]==\"\"][\"character\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'wnut.txt', wnut.values, fmt='%s', delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "conatenated_dev=pd.concat([columns_dev,wnut],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'conatenated_dev.txt', conatenated_dev.values, fmt='%s', delimiter=' ',encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing sentences with extra spaces between actual word and 's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"'s\" in split_sentence:\n",
    "    if \"]\" in split_sentence[split_sentence.index(\"'s\")-1]:\n",
    "        split_sentence[split_sentence.index(\"'s\")-1]=split_sentence[split_sentence.index(\"'s\")-1][:-1]+\"'s\"+\"]\"\n",
    "    else:\n",
    "        split_sentence[split_sentence.index(\"'s\")-1]=split_sentence[split_sentence.index(\"'s\")-1]+\"'s\"\n",
    "    split_sentence.remove(\"'s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
